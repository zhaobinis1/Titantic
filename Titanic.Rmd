---
title: "Titanic Survival Predictions on Kaggle"
author: "Bin Zhao"
date: "March 29, 2016"
output: html_document
---

### About This Article
This is a demonstration of my way of solving the Titanic survival prediction problem on [kaggle.com](http://www.kaggle.com). R is used for all the data munging, visualization and analysis.  

### Background
The main objective is to predict whether a passenger would survive the Titanic accident based on their attributes, such as sex, age, class and cabin location. There is a common hypothesis that one's survival depends heavily on distance to lifeboat when accident happened. Cabin locations for each passenger could be very useful in prediction as the accident happened around 11:40 pm and most passengers should be in their cabins. In addition, "women and children first" is believed to be the protocol followed when getting people onto the lifeboats. Thus sex and age may be important as well. We will explore more predictors that are potentially useful in the next section.

### Read the data
Two data sets are obtained from Kaggle:  

* __train.csv__ (contains attributes and outcomes [survived or not])  
* __test.csv__ (contains attributes only without outcomes)

Knowing what types of variables are included in the data set before you read them into R is very important and could keep you away from lots of troubles in future steps. Sometimes I would just read in the data mindlessly and have a quick look at the data.   
```{r}
tr=read.csv("train.csv")
summary(tr)
```
It is clear some variables, such as "Survived" and "Pclass", are categorical; some variables, such as "age", are continuous. Also note that there are some blank entries in the data. So we need to specify missing values as "NA" or "".
```{r}
tr=read.csv("train.csv",na.strings = c("NA",""),colClasses=c('integer',   # PassengerId
                                                             'factor',    # Survived 
                                                             'factor',    # Pclass
                                                             'character', # Name
                                                             'factor',    # Sex
                                                             'numeric',   # Age
                                                             'integer',   # SibSp
                                                             'integer',   # Parch
                                                             'character', # Ticket
                                                             'numeric',   # Fare
                                                             'character', # Cabin
                                                             'factor'     # Embarked
)
)
te=read.csv("test.csv",na.strings = c("NA",""),colClasses=c('integer',   # PassengerId
                                                            'factor',    # Pclass
                                                            'character', # Name
                                                            'factor',    # Sex
                                                            'numeric',   # Age
                                                            'integer',   # SibSp
                                                            'integer',   # Parch
                                                            'character', # Ticket
                                                            'numeric',   # Fare
                                                            'character', # Cabin
                                                            'factor'     # Embarked
)
)
```
### Data munging
Now let's look at the missing data. We use ```missmap``` function from Amelia package.
```{r}
library(Amelia)
missmap(tr,legend=F)
missmap(te,legend=F)
```

In the training set, it is very unfortunate that many of the cabin locations are missing, as we mentioned before, this could have been very useful in prediction. Age also has roughly 20% of missing values. The percentage of missing values in cabin number is too large to make some meaningful imputation. We will try to "recover" missing age and embarkment later. The story is similar for test set.

Now let's do some data visualization. ggplot2 is the package that is going to help us here.
```{r}
library(ggplot2)
library(gridExtra)
p1=qplot(x=Survived,data=tr,geom="bar")
p2=qplot(x=Pclass,data=tr,geom="bar")
p3=qplot(x=Sex,data=tr,geom="bar")
p4=qplot(x=Age,data=tr,geom="histogram")
p5=qplot(x=Parch,data=tr,geom="bar",binwidth = 0.5)
p6=qplot(x=SibSp,data=tr,geom="bar",binwidth = 0.5)
grid.arrange(p1,p2,p3,p4,p5,p6,ncol=3)
```

A few observations from the plots above:

* More people died than survived
* More male than female
* More third class than first and second class

Next, we use mosaic plots to explore associations between survival and other variables. The function used is ```mosaicplot``` from package vcd.
```{r}
library(vcd)
mosaicplot(Pclass~Survived,data=tr,shade=F,color=T,main="class and survival")
```

Apparently, third class passengers have a lower odds of survival. Travelling class is an important predictor.

```{r}
mosaicplot(Sex~Survived,data=tr,shade=F,color=T,main="Sex and survival")
```

Sex is also very important, as we can see the survival rate of female is much larger than that of male. This is evidence for "women and children first".

Now we deal with missing values. Combine training and testing set so we can do some imputations.
```{r}
library(dplyr)
full=bind_rows(tr,te)
```

For two missing embarkment, notice that they both have fare $80. We then assign them to "C".
```{r}
full$Embarked[is.na(full$Embarked)]="C"
```

The pasenger with missing fare is a third class passenger embarked from "S". Plot the distribution of fare for all the thrid class passengers from "S", we replace the missing value with the median of fare.
```{r}
ggplot(full[full$Pclass == '3' & full$Embarked == 'S', ], aes(x = Fare)) + geom_density(fill = '#99d6ff', alpha=0.4) + geom_vline(aes(xintercept=median(Fare,na.rm=T)),colour='red', linetype='dashed', lwd=1)
full$Fare[1044] <- median(full[full$Pclass == '3' & full$Embarked == 'S', ]$Fare, na.rm = TRUE)
```

We use random forest to impute missing ages.
```{r}
library(randomForest)
set.seed(1234)
#impute age using random forest
imputefit=randomForest(Age~.,data=full[,-c(1,2,4,9,11)][! is.na(full$Age),],na.action = na.omit,xtest=full[,-c(1,2,4,6,9,11)][is.na(full$Age),],ntree=5000)
full$Age[is.na(full$Age)]=imputefit$test$predicted
```

One interesting column is Name. Let us take a look.
```{r}
head(full$Name)
```

We will get titles from names as they are potentially useful for prediction.
```{r}
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
# Titles with very low cell counts to be combined to "rare" level
rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 
                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')
# Also reassign mlle, ms, and mme accordingly
full$Title[full$Title == 'Mlle']        <- 'Miss' 
full$Title[full$Title == 'Ms']          <- 'Miss'
full$Title[full$Title == 'Mme']         <- 'Mrs' 
full$Title[full$Title %in% rare_title]  <- 'Rare'
#change factors levels name
levels(full$Survived)=c("no","yes")
#split into train and test
tr1=full[1:891,]
te1=full[892:nrow(full),]
#keep variables of useful
usefulvar=c("Survived"  ,  "Pclass"    ,   "Sex"    ,     "Age"      ,   "SibSp"   ,    "Parch"   ,
            "Fare"      ,   "Embarked"  ,  "Title" )
tr1=tr1[,usefulvar]
te1=te1[,usefulvar]
te1=te1[,-1]
```

### Fitting Models and stacking

Stacking will be used in this section. Stacking is a technique to combine several individual prediction models. We will use package ###caret### and ###h2o###. Models used are random forest, 2 layer nuetral network, logistic regression, adaboost and SVM. Below is a brief discription of how stacking works.

k-fold stacking: 

*Split the train set into k parts: train_1 to train_k
*Do the usual cv fit: Fit a model on all training data excluding train_a, get prediction on train_a. 
*Then we will have cv predictions on all training data. 
*Repeat for other first-stage models. Suppose we have L such models.
*We would have L vectors of predictions of length n.
*Use this n*L matrix as design matrix and true response of training data as "Y" to get another second-stage model.
*Finally, fit each of L models again on full training data. They are called L first-stage models.
*the second-stage model and L first-stage models will be used on test data to get the final prediction.

In this case, we will use 2-fold cross validation to get predictions.
```{r}
library(caret)
#We will do a 2-fold cv to get predictions
#slice training data into 2
set.seed(1234)
intrain1=createDataPartition(tr1$Survived,list=F)
train1=tr1[intrain1,]
train2=tr1[-intrain1,]
#prepare variables to contain predictions from each of 5 models.
pred1=rep(0,nrow(tr1))
pred2=pred1
pred3=pred1
pred4=pred1
pred5=pred1
```

We will also specify a control method for each model.
```{r}
#control method
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 3,
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE)
```
For random forest, the only tuning parameter is "mtry", i.e. number of variables selected at each node. Since we only have very few variables, no need to tune it.
```{r}
set.seed(1234)
fit1_1=train(Survived~.,data=train1,metric = "ROC",method="rf",trControl = cv.ctrl)
fit1_2=train(Survived~.,data=train2,metric = "ROC",method="rf",trControl = cv.ctrl)
pred1_1=predict(fit1_1,train2[,-1])
pred1_2=predict(fit1_2,train1[,-1])
pred1[intrain1]=pred1_2
pred1[-intrain1]=pred1_1
pred1=pred1-1
```

Next we fit the 2 layer neutral network.
```{r}
library(h2o)
## start a local h2o cluster
localH2O = h2o.init(max_mem_size = '6g', nthreads = -1) # use all CPUs (8 on my personal computer)
## convert data to H2O data object
train_h2o = as.h2o(train1)
## train model
set.seed(1234)
fit2_1 =
  h2o.deeplearning(x = 2:9,  # column numbers for predictors
                   y = 1,   # column number for label
                   training_frame = train_h2o, # data in H2O format
                   activation = "RectifierWithDropout", # algorithm
                   input_dropout_ratio = 0.2, # % of inputs dropout
                   hidden_dropout_ratios = c(0.5,0.5), # % for nodes dropout
                   balance_classes = TRUE, 
                   hidden = c(100,100), # two layers of 100 nodes
                   momentum_stable = 0.99,
                   nesterov_accelerated_gradient = T, # use it for speed
                   epochs = 15) # no. of epochs
test_h2o=as.h2o(train2[,-1])
## classify test set
h2o_y_test <- h2o.predict(fit2_1, test_h2o)
## convert H2O format into data frame and  save as csv
df_y_test = as.data.frame(h2o_y_test)
pred2_1=df_y_test[,1]

train_h2o = as.h2o(train2)
set.seed(1234)
fit2_2 =
  h2o.deeplearning(x = 2:9,  # column numbers for predictors
                   y = 1,   # column number for label
                   training_frame = train_h2o, # data in H2O format
                   activation = "RectifierWithDropout", # algorithm
                   input_dropout_ratio = 0.2, # % of inputs dropout
                   hidden_dropout_ratios = c(0.5,0.5), # % for nodes dropout
                   balance_classes = TRUE, 
                   hidden = c(100,100), # two layers of 100 nodes
                   momentum_stable = 0.99,
                   nesterov_accelerated_gradient = T, # use it for speed
                   epochs = 15) # no. of epochs
test_h2o=as.h2o(train1[,-1])
# classify test set
h2o_y_test <- h2o.predict(fit2_2, test_h2o)
## convert H2O format into data frame and  save as csv
df_y_test = as.data.frame(h2o_y_test)
pred2_2=df_y_test[,1]
pred2[intrain1]=pred2_2
pred2[-intrain1]=pred2_1
pred2=pred2-1
## shut down virutal H2O cluster
h2o.shutdown(prompt = F)
```

Next, logistic regression.
```{r}
#fit glm
set.seed(1234)
fit3_1=train(Survived~.,data=train1,method="glm",metric = "ROC", 
           trControl = cv.ctrl)
fit3_2=train(Survived~.,data=train2,method="glm",metric = "ROC", 
             trControl = cv.ctrl)
pred3_1=predict(fit3_1,newdata=train2[,-1])
pred3_2=predict(fit3_2,newdata=train1[,-1])
pred3[intrain1]=pred3_2
pred3[-intrain1]=pred3_1
pred3=pred3-1
```

Next, adaboost.
```{r}
#fit boosting
#adaboost has 3 tuning parameters: iter maxdepth and nu (shrikage parameter)
ada.grid <- expand.grid(.iter = c(50, 100),
                        .maxdepth = c(4, 8),
                        .nu = c(0.1, 1))
set.seed(1234)
fit4_1 <- train(Survived~., 
              data = train1,
              method = "ada",
              metric = "ROC",
              tuneGrid = ada.grid,
              trControl = cv.ctrl)
fit4_2 <- train(Survived~., 
                data = train2,
                method = "ada",
                metric = "ROC",
                tuneGrid = ada.grid,
                trControl = cv.ctrl)
pred4_1=predict(fit4_1,train2[,-1])
pred4_2=predict(fit4_2,train1[,-1])
pred4[intrain1]=pred4_2
pred4[-intrain1]=pred4_1
pred4=pred4-1
```
Finally, SVM
```{r}
#fit svm
set.seed(1234)
fit5_1 <- train(Survived~., 
              data = train1,
              method = "svmRadial",
              tuneLength = 9,#tune cost parameter. length=9 means we will test C=c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64)
              preProcess = c("center", "scale"),#normalize data as svm is sensitive to scale.normalization happens in each resample loop.
              metric = "ROC",
              trControl = cv.ctrl)
fit5_2 <- train(Survived~., 
                data = train2,
                method = "svmRadial",
                tuneLength = 9,#tune cost parameter. length=9 means we will test C=c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64)
                preProcess = c("center", "scale"),#normalize data as svm is sensitive to scale.normalization happens in each resample loop.
                metric = "ROC",
                trControl = cv.ctrl)
pred5_1=predict(fit5_1,train2[,-1])
pred5_2=predict(fit5_2,train1[,-1])
pred5[intrain1]=pred5_2
pred5[-intrain1]=pred5_1
pred5=pred5-1
```

Now that we have cv predictions from 5 models, we combine them into an n*5 matrix along with true y values from training set, then fit a logistic regression (second-stage model).
```{r}
dat=cbind(tr1$Survived,pred1,pred2,pred3,pred4,pred5)
dat[,1]=dat[,1]-1
colnames(dat)[1]="Survived"
dat=data.frame(dat)
dat[,1:6]=lapply(dat[,1:6],factor)
for(i in 1:6){
levels(dat[,i])=c("no","yes")
}
#second-stage model
superfit=train(as.factor(Survived)~.,data=dat,method="glm",metric = "ROC", 
                    trControl = cv.ctrl)
#also need 5 first-stage models fitted on full training data
set.seed(1234)
fit1=train(Survived~.,data=tr1,metric = "ROC",method="rf",trControl = cv.ctrl)

localH2O = h2o.init(max_mem_size = '6g', nthreads = -1) # use all CPUs (8 on my personal computer)
train_h2o = as.h2o(tr1)
set.seed(1234)
fit2 =
  h2o.deeplearning(x = 2:9,  # column numbers for predictors
                   y = 1,   # column number for label
                   training_frame = train_h2o, # data in H2O format
                   activation = "RectifierWithDropout", # algorithm
                   input_dropout_ratio = 0.2, # % of inputs dropout
                   hidden_dropout_ratios = c(0.5,0.5), # % for nodes dropout
                   balance_classes = TRUE, 
                   hidden = c(100,100), # two layers of 100 nodes
                   momentum_stable = 0.99,
                   nesterov_accelerated_gradient = T, # use it for speed
                   epochs = 15) # no. of epochs

set.seed(1234)
fit3=train(Survived~.,data=tr1,method="glm",metric = "ROC", 
             trControl = cv.ctrl)

set.seed(1234)
fit4 <- train(Survived~., 
                data = tr1,
                method = "ada",
                metric = "ROC",
                tuneGrid = ada.grid,
                trControl = cv.ctrl)

set.seed(1234)
fit5 <- train(Survived~., 
                data = tr1,
                method = "svmRadial",
                tuneLength = 9,#tune cost parameter. length=9 means we will test C=c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64)
                preProcess = c("center", "scale"),#normalize data as svm is sensitive to scale.normalization happens in each resample loop.
                metric = "ROC",
                trControl = cv.ctrl)

#get predicitons on test set
preddf=data.frame(matrix(nrow=418,ncol=5))
preddf[,1]=predict(fit1,te1)

test_h2o=as.h2o(te1)
## classify test set
h2o_y_test <- h2o.predict(fit2, test_h2o)
## convert H2O format into data frame and  save as csv
df_y_test = as.data.frame(h2o_y_test)
preddf[,2]=df_y_test[,1]

preddf[,3]=predict(fit3,te1)
preddf[,4]=predict(fit4,te1)
preddf[,5]=predict(fit5,te1)
colnames(preddf)=c("pred1","pred2","pred3","pred4","pred5")
#get final predictions with second-stage model, where covariates are predictions from the 5 first-stage model on test data
finalpred=predict(superfit,newdata=preddf)
```

Convert the result into the form Kaggle accept.
```{r,echo=F}
Survived=as.numeric(finalpred)-1
PassengerId=te$PassengerId
result1=data.frame(PassengerId,Survived)
library(readr)
write_csv(result1,"result1.csv")
```
